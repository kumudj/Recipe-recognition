{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85f1de28",
   "metadata": {},
   "source": [
    "# Project Flow"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b9f9f30e",
   "metadata": {},
   "source": [
    "Data Collection\n",
    "   Collect the dataset or Create the dataset\n",
    "\n",
    "Data Preprocessing\n",
    "   Import the ImageDataGenerator library\n",
    "   Configure ImageDataGenerator class\n",
    "   Apply ImageDataGenerator functionality to Trainset and Testset\n",
    "\n",
    "Model Building\n",
    "   Import the model building Libraries\n",
    "   Initializing the model\n",
    "   Adding Input Layer\n",
    "   Adding Hidden Layer\n",
    "   Adding Output Layer\n",
    "   Configure the Learning Process\n",
    "   Training the model\n",
    "   Save the Model\n",
    "   Test the Model\n",
    "\n",
    "Application Building\n",
    "   Create an HTML file\n",
    "   Build Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "052d4aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np#used for numerical analysis\n",
    "import tensorflow #open source used for both ML and DL for computation\n",
    "from tensorflow.keras.models import Sequential #it is a plain stack of layers\n",
    "from tensorflow.keras import layers #A layer consists of a tensor-in tensor-out computation function\n",
    "#Dense layer is the regular deeply connected neural network layer\n",
    "from tensorflow.keras.layers import Dense,Flatten\n",
    "#Faltten-used fot flattening the input or change the dimension\n",
    "from tensorflow.keras.layers import Convolution2D,MaxPooling2D #Convolutional layer\n",
    "#MaxPooling2D-for downsampling the image\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d55a101",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting parameter for Image Data agumentation to the traing data\n",
    "train_datagen=ImageDataGenerator(rescale=1./255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04b6449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image Data agumentation to the testing data\n",
    "test_datagen=ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd025640",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2418 images belonging to 3 classes.\n",
      "Found 600 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "#performing data agumentation to train data\n",
    "x_train=train_datagen.flow_from_directory(directory=r'C:\\Users\\kumud\\Downloads\\Food recipe\\dataset\\training_set'\n",
    "                                          ,target_size=(64,64),batch_size=32,class_mode='categorical')\n",
    "#performing data agumentation to test data\n",
    "x_test=test_datagen.flow_from_directory(directory=r'C:\\Users\\kumud\\Downloads\\Food recipe\\dataset\\test_set'\n",
    "                                        ,target_size=(64,64),batch_size=32,class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7de419f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'french_fries': 0, 'pizza': 1, 'samosa': 2}\n"
     ]
    }
   ],
   "source": [
    "print(x_train.class_indices)#checking the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8935440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 804, 1: 804, 2: 810})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter as c\n",
    "c(x_train.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "088aeb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model=Sequential()\n",
    "# adding model layer\n",
    "model.add(Convolution2D(32,(3,3),input_shape=(64,64,3),activation='relu'))#convolutional layer\n",
    "model.add(MaxPooling2D(pool_size=(2,2))) #MaxPooling2D-for downsampling the input\n",
    "\n",
    "model.add(Convolution2D(32,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())#flatten the dimension of the image\n",
    "model.add(Dense(32))#deeply connected neural network layers.\n",
    "model.add(Dense(3,activation='softmax'))#output layer with 3 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cdf469c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 31, 31, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 29, 29, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 14, 14, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6272)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                200736    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 210,979\n",
      "Trainable params: 210,979\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()#summary of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c68a4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6684365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-d8626421742f>:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(generator=x_train,steps_per_epoch = len(x_train),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "76/76 [==============================] - 27s 346ms/step - loss: 1.1469 - accuracy: 0.3420 - val_loss: 1.0960 - val_accuracy: 0.3750\n",
      "Epoch 2/40\n",
      "76/76 [==============================] - 24s 319ms/step - loss: 1.0874 - accuracy: 0.4045 - val_loss: 1.0850 - val_accuracy: 0.3833\n",
      "Epoch 3/40\n",
      "76/76 [==============================] - 24s 319ms/step - loss: 1.0410 - accuracy: 0.4524 - val_loss: 0.9080 - val_accuracy: 0.5667\n",
      "Epoch 4/40\n",
      "76/76 [==============================] - 24s 313ms/step - loss: 0.9364 - accuracy: 0.5488 - val_loss: 0.8918 - val_accuracy: 0.5750\n",
      "Epoch 5/40\n",
      "76/76 [==============================] - 25s 323ms/step - loss: 0.9079 - accuracy: 0.5720 - val_loss: 0.8479 - val_accuracy: 0.6100\n",
      "Epoch 6/40\n",
      "76/76 [==============================] - 24s 316ms/step - loss: 0.8567 - accuracy: 0.6038 - val_loss: 0.9500 - val_accuracy: 0.5383\n",
      "Epoch 7/40\n",
      "76/76 [==============================] - 24s 315ms/step - loss: 0.8321 - accuracy: 0.6199 - val_loss: 0.8689 - val_accuracy: 0.5750\n",
      "Epoch 8/40\n",
      "76/76 [==============================] - 24s 313ms/step - loss: 0.7932 - accuracy: 0.6406 - val_loss: 0.7491 - val_accuracy: 0.6717\n",
      "Epoch 9/40\n",
      "76/76 [==============================] - 25s 323ms/step - loss: 0.7469 - accuracy: 0.6625 - val_loss: 1.0412 - val_accuracy: 0.5350\n",
      "Epoch 10/40\n",
      "76/76 [==============================] - 23s 301ms/step - loss: 0.7242 - accuracy: 0.6882 - val_loss: 0.8448 - val_accuracy: 0.6250\n",
      "Epoch 11/40\n",
      "76/76 [==============================] - 26s 343ms/step - loss: 0.7287 - accuracy: 0.6902 - val_loss: 0.8113 - val_accuracy: 0.6567\n",
      "Epoch 12/40\n",
      "76/76 [==============================] - 24s 312ms/step - loss: 0.6627 - accuracy: 0.7184 - val_loss: 0.6613 - val_accuracy: 0.7167\n",
      "Epoch 13/40\n",
      "76/76 [==============================] - 23s 306ms/step - loss: 0.6488 - accuracy: 0.7217 - val_loss: 0.6750 - val_accuracy: 0.7250\n",
      "Epoch 14/40\n",
      "76/76 [==============================] - 22s 288ms/step - loss: 0.6357 - accuracy: 0.7316 - val_loss: 0.7339 - val_accuracy: 0.6950\n",
      "Epoch 15/40\n",
      "76/76 [==============================] - 23s 300ms/step - loss: 0.6069 - accuracy: 0.7519 - val_loss: 0.6539 - val_accuracy: 0.7283\n",
      "Epoch 16/40\n",
      "76/76 [==============================] - 23s 300ms/step - loss: 0.6454 - accuracy: 0.7337 - val_loss: 0.6423 - val_accuracy: 0.7350\n",
      "Epoch 17/40\n",
      "76/76 [==============================] - 24s 315ms/step - loss: 0.6411 - accuracy: 0.7320 - val_loss: 0.6175 - val_accuracy: 0.7450\n",
      "Epoch 18/40\n",
      "76/76 [==============================] - 23s 308ms/step - loss: 0.6036 - accuracy: 0.7411 - val_loss: 0.6717 - val_accuracy: 0.7267\n",
      "Epoch 19/40\n",
      "76/76 [==============================] - 24s 314ms/step - loss: 0.5765 - accuracy: 0.7663 - val_loss: 0.6073 - val_accuracy: 0.7500\n",
      "Epoch 20/40\n",
      "76/76 [==============================] - 24s 310ms/step - loss: 0.5843 - accuracy: 0.7539 - val_loss: 0.6210 - val_accuracy: 0.7500\n",
      "Epoch 21/40\n",
      "76/76 [==============================] - 23s 308ms/step - loss: 0.5609 - accuracy: 0.7680 - val_loss: 0.6357 - val_accuracy: 0.7200\n",
      "Epoch 22/40\n",
      "76/76 [==============================] - 25s 327ms/step - loss: 0.5637 - accuracy: 0.7605 - val_loss: 0.5837 - val_accuracy: 0.7600\n",
      "Epoch 23/40\n",
      "76/76 [==============================] - 26s 338ms/step - loss: 0.5502 - accuracy: 0.7796 - val_loss: 0.6097 - val_accuracy: 0.7617\n",
      "Epoch 24/40\n",
      "76/76 [==============================] - 23s 298ms/step - loss: 0.5327 - accuracy: 0.7738 - val_loss: 0.6705 - val_accuracy: 0.7250\n",
      "Epoch 25/40\n",
      "76/76 [==============================] - 25s 324ms/step - loss: 0.5370 - accuracy: 0.7816 - val_loss: 0.6114 - val_accuracy: 0.7550\n",
      "Epoch 26/40\n",
      "76/76 [==============================] - 24s 320ms/step - loss: 0.5270 - accuracy: 0.7775 - val_loss: 0.6260 - val_accuracy: 0.7267\n",
      "Epoch 27/40\n",
      "76/76 [==============================] - 24s 314ms/step - loss: 0.5194 - accuracy: 0.7858 - val_loss: 0.5585 - val_accuracy: 0.7633\n",
      "Epoch 28/40\n",
      "76/76 [==============================] - 24s 320ms/step - loss: 0.5094 - accuracy: 0.7887 - val_loss: 0.6165 - val_accuracy: 0.7633\n",
      "Epoch 29/40\n",
      "76/76 [==============================] - 23s 307ms/step - loss: 0.4950 - accuracy: 0.7949 - val_loss: 0.5883 - val_accuracy: 0.7767\n",
      "Epoch 30/40\n",
      "76/76 [==============================] - 26s 338ms/step - loss: 0.5364 - accuracy: 0.7750 - val_loss: 0.5863 - val_accuracy: 0.7817\n",
      "Epoch 31/40\n",
      "76/76 [==============================] - 24s 321ms/step - loss: 0.4970 - accuracy: 0.7978 - val_loss: 0.5702 - val_accuracy: 0.7683\n",
      "Epoch 32/40\n",
      "76/76 [==============================] - 22s 293ms/step - loss: 0.4320 - accuracy: 0.8342 - val_loss: 0.6027 - val_accuracy: 0.7817\n",
      "Epoch 33/40\n",
      "76/76 [==============================] - 23s 301ms/step - loss: 0.4669 - accuracy: 0.8139 - val_loss: 0.6131 - val_accuracy: 0.7700\n",
      "Epoch 34/40\n",
      "76/76 [==============================] - 23s 305ms/step - loss: 0.4779 - accuracy: 0.8031 - val_loss: 0.7031 - val_accuracy: 0.7333\n",
      "Epoch 35/40\n",
      "76/76 [==============================] - 25s 325ms/step - loss: 0.4348 - accuracy: 0.8234 - val_loss: 0.6611 - val_accuracy: 0.7833\n",
      "Epoch 36/40\n",
      "76/76 [==============================] - 24s 317ms/step - loss: 0.4582 - accuracy: 0.8131 - val_loss: 0.5714 - val_accuracy: 0.7983\n",
      "Epoch 37/40\n",
      "76/76 [==============================] - 23s 300ms/step - loss: 0.4406 - accuracy: 0.8255 - val_loss: 0.5510 - val_accuracy: 0.8017\n",
      "Epoch 38/40\n",
      "76/76 [==============================] - 25s 332ms/step - loss: 0.4255 - accuracy: 0.8342 - val_loss: 0.5939 - val_accuracy: 0.8067\n",
      "Epoch 39/40\n",
      "76/76 [==============================] - 23s 308ms/step - loss: 0.4015 - accuracy: 0.8420 - val_loss: 0.6016 - val_accuracy: 0.7850\n",
      "Epoch 40/40\n",
      "76/76 [==============================] - 24s 318ms/step - loss: 0.4243 - accuracy: 0.8325 - val_loss: 0.6146 - val_accuracy: 0.7950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f68fc92b50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit_generator(generator=x_train,steps_per_epoch = len(x_train),\n",
    "                    epochs=40, validation_data=x_test,validation_steps = len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9ee48a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('new_food.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39901537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "model = load_model(\"new_food.h5\") #loading the model for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5311969",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\Users\\\\kumud\\\\Downloads\\\\Food recipe\\\\dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-42b91904bfa0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\kumud\\Downloads\\Food recipe\\dataset\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#loading of the image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#image to array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#changing the shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#predicting the classes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[0;32m    311\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msupported\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m   \"\"\"\n\u001b[1;32m--> 313\u001b[1;33m   return image.load_img(path, grayscale=grayscale, color_mode=color_mode,\n\u001b[0m\u001b[0;32m    314\u001b[0m                         target_size=target_size, interpolation=interpolation)\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\utils.py\u001b[0m in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[0;32m    111\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[0;32m    112\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[1;32m--> 113\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'grayscale'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:\\\\Users\\\\kumud\\\\Downloads\\\\Food recipe\\\\dataset'"
     ]
    }
   ],
   "source": [
    "img = image.load_img(r\"C:\\Users\\kumud\\Downloads\\Food recipe\\dataset\",target_size= (64,64))#loading of the image\n",
    "x = image.img_to_array(img)#image to array\n",
    "x = np.expand_dims(x,axis = 0)#changing the shape\n",
    "pred = model.predict(x)#predicting the classes\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badb8df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "index=['french_fries', 'pizza', 'samosa']\n",
    "result=str(index[np.argmax(pred)])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7392da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830dda10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b516295b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
